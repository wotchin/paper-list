# 介绍
此处介绍经典的机器学习理论论文，仅涉及通用算法部分，细分领域参见其他部分。

# Deep Learning
|Name|Comment|Published Year|
|-|-|-|
|Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift|批归一化方法，能够显著提升模型性能。|2015|
|Improving neural networks by preventing co-adaptation of feature detectors|Hinton首次提出Dropout，能够防止过拟合，提高训练模型的泛化能力。|2012|
|Dropout: A Simple Way to Prevent Neural Networks from Overfitting|Dropout补充|2014|
|Improving Neural Network Generalization by Combining Parallel Circuits with Dropout|Dropout补充|2016|
|Dropout as data augmentation|Dropout补充|2015|
|Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)|ELU激活函数|2015|
|Self-Normalizing Neural Networks|提出SELUs激活函数，在FNN中表现优异|2017|
|[Deep Learning](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf)|Lecun, Hinton, Bengio联合发表的一篇综述文章,推荐阅读。|2015|
|[Deep learning in neural networks](https://arxiv.org/pdf/1404.7828.pdf)|LSTM之父Juergen Schmidhuber撰写的综述|2015|
|Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks|DCGAN的实现，为使用GAN生成高仿真图片拓宽了道路。|2015|